{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/pranavgajera/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/pranavgajera/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from embeddings import GloveEmbedding, FastTextEmbedding, KazumaCharEmbedding, ConcatEmbedding\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nlppreprocess import NLP\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings = GloveEmbedding('common_crawl_840', d_emb=300, show_progress=True,default='zero' )\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nlp = NLP()\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "negative_stopwords = [\"no\", \"not\", \"never\", \"none\", \"nothing\", \"nobody\"]\n",
    "stopwords = [word for word in stopwords if word not in negative_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset=1):\n",
    "    if(dataset==1):\n",
    "        train_df = pd.read_excel('/Users/pranavgajera/Downloads/DLNLP/Assignment1/Dataset/ClassificationDataset-train0.xlsx',header=None).dropna()\n",
    "        val_df = pd.read_excel('/Users/pranavgajera/Downloads/DLNLP/Assignment1/Dataset/ClassificationDataset-valid0.xlsx',header=None).dropna()\n",
    "        train_df.columns = ['target','sentiment']\n",
    "        val_df.columns = ['target','sentiment']\n",
    "    elif(dataset==2):\n",
    "        train_df = pd.read_excel('/Users/pranavgajera/Downloads/DLNLP/Assignment1/Dataset/ClassificationDataset-train1.xlsx').dropna()\n",
    "        val_df = pd.read_excel('/Users/pranavgajera/Downloads/DLNLP/Assignment1/Dataset/ClassificationDataset-valid1.xlsx').dropna()\n",
    "    elif(dataset==3):\n",
    "        train_df = pd.read_excel('/Users/pranavgajera/Downloads/DLNLP/Assignment1/Dataset/ClassificationDataset-train2.xlsx').dropna()\n",
    "        val_df = pd.read_excel('/Users/pranavgajera/Downloads/DLNLP/Assignment1/Dataset/ClassificationDataset-valid2.xlsx').dropna()\n",
    "        \n",
    "    return  train_df,val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_to_cummulative_embedding(message):\n",
    "    tokens = tokenizer.tokenize(nlp.process(message))\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    useful_words = [t for t in lemmatized_tokens if t.lower() not in stopwords]\n",
    "    \n",
    "    embedding_vector = np.zeros((glove_embeddings.d_emb))\n",
    "    for word in useful_words:\n",
    "        word_embedding = glove_embeddings.emb(word)\n",
    "        embedding_vector = np.add(embedding_vector,word_embedding)\n",
    "    \n",
    "    return embedding_vector\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataset(dataset=1):\n",
    "    train_df, val_df = load_dataset(dataset)\n",
    "\n",
    "    X_train = train_df.iloc[:,1]\n",
    "    y_train = train_df.iloc[:,0]\n",
    "    X_val = val_df.iloc[:,1]\n",
    "    y_val = val_df.iloc[:,0]\n",
    "\n",
    "    if(dataset==1):\n",
    "        y_train = [np.array([1,0,0]) if y=='negative' else np.array([0,1,0]) if y=='neutral' else np.array([0,0,1]) for y in y_train]\n",
    "        y_val = [np.array([1,0,0]) if y=='negative' else np.array([0,1,0]) if y=='neutral' else np.array([0,0,1]) for y in y_val]\n",
    "    elif(dataset==2):\n",
    "        y_train = [np.array([1,0]) if y==0 else np.array([0,1]) for y in y_train]\n",
    "        y_val = [np.array([1,0]) if y==0 else np.array([0,1]) for y in y_val]\n",
    "    elif(dataset==3):\n",
    "        y_train = [np.array([1,0,0,0,0]) if y==1 else np.array([0,1,0,0,0]) if y==2 else np.array([0,0,1,0,0]) if y==3 else np.array([0,0,0,1,0]) if y==4 else np.array([0,0,0,0,1]) for y in y_train]\n",
    "        y_val = [np.array([1,0,0,0,0]) if y==1 else np.array([0,1,0,0,0]) if y==2 else np.array([0,0,1,0,0]) if y==3 else np.array([0,0,0,1,0]) if y==4 else np.array([0,0,0,0,1]) for y in y_val]\n",
    "\n",
    "    X_train = [message_to_cummulative_embedding(m) for m in X_train]\n",
    "    X_val = [message_to_cummulative_embedding(m) for m in X_val]\n",
    "\n",
    "    return np.array(X_train),np.array(X_val),np.array(y_train),np.array(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DANSentimentClassifier(nn.Module):\n",
    "    def __init__(self,input_dim,hidden1_dim,hidden2_dim,hidden3_dim,drop_out1,drop_out2,drop_out3,num_classes):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_dim,hidden1_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(drop_out1)\n",
    "        self.layer2 = nn.Linear(hidden1_dim,hidden2_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(drop_out2)\n",
    "        self.layer3 = nn.Linear(hidden2_dim,hidden3_dim)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(drop_out3)\n",
    "        self.out = nn.Linear(hidden3_dim,num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = F.softmax(self.out(x),dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, epochs, lr):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_accu = list()\n",
    "    val_acuu = list()\n",
    "    epoch_list = list()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_list.append(epoch+1)\n",
    "        correct = 0\n",
    "        inputs, labels = train_data\n",
    "        inputs = torch.tensor(inputs,dtype=torch.float32)\n",
    "        labels = torch.tensor(labels,dtype=torch.float32)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        predictions = outputs.argmax(dim=-1)\n",
    "        labels = labels.argmax(dim=-1)\n",
    "        correct += (predictions==labels).sum().item()\n",
    "        train_accuracy = correct/inputs.shape[0]\n",
    "        train_accu.append(train_accuracy)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = val_data\n",
    "            inputs = torch.tensor(inputs,dtype=torch.float32)\n",
    "            labels = torch.tensor(labels,dtype=torch.float32)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            predictions = outputs.argmax(dim=-1)\n",
    "            labels = labels.argmax(dim=-1)\n",
    "            correct += (predictions==labels).sum().item()\n",
    "            val_accuracy = correct/inputs.shape[0]\n",
    "            val_acuu.append(val_accuracy)\n",
    "            f1_score = sklearn.metrics.f1_score(labels, predictions,average='micro')\n",
    "            confusion_matrix = sklearn.metrics.confusion_matrix(labels, predictions)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Train loss {loss}, Val loss {val_loss}, Train acc {train_accuracy} , Val acc {val_accuracy}, F1-Score {f1_score}\")\n",
    "    print(\"Confusion Matrix :\")\n",
    "    print(confusion_matrix)\n",
    "    print()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}: Train loss {loss}, Val loss {val_loss}, Train acc {train_accuracy} , Val acc {val_accuracy}, F1-Score {f1_score}\")\n",
    "\n",
    "    plt.plot(epoch_list, train_accu)\n",
    "    plt.plot(epoch_list,val_acuu)\n",
    "    plt.xlabel(\"Epochs\") \n",
    "    plt.ylabel(\"Acccuracy\") \n",
    "    plt.title(f\"Train and Validation Accuracy for dataset {dataset}\") \n",
    "    plt.legend([\"Train\",\"Validation\"])\n",
    "    plt.savefig(f'dataset_{dataset}.png')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/350: Train loss 1.638727068901062, Val loss 1.5978288650512695, Train acc 0.10368251698248122 , Val acc 0.2006661115736886, F1-Score 0.2006661115736886\n",
      "Epoch 2/350: Train loss 1.5936328172683716, Val loss 1.5442951917648315, Train acc 0.20843761172685019 , Val acc 0.4113238967527061, F1-Score 0.4113238967527061\n",
      "Epoch 3/350: Train loss 1.5522042512893677, Val loss 1.500227928161621, Train acc 0.4004290311047551 , Val acc 0.5611990008326395, F1-Score 0.5611990008326395\n",
      "Epoch 4/350: Train loss 1.5013766288757324, Val loss 1.4471617937088013, Train acc 0.5423668215945656 , Val acc 0.649458784346378, F1-Score 0.649458784346378\n",
      "Epoch 5/350: Train loss 1.4418368339538574, Val loss 1.3819689750671387, Train acc 0.654987486592778 , Val acc 0.7110741049125728, F1-Score 0.7110741049125728\n",
      "Epoch 6/350: Train loss 1.376856803894043, Val loss 1.3138515949249268, Train acc 0.7222023596710762 , Val acc 0.7485428809325562, F1-Score 0.7485428809325563\n",
      "Epoch 7/350: Train loss 1.305350661277771, Val loss 1.2565040588378906, Train acc 0.7618877368609224 , Val acc 0.7693588676103247, F1-Score 0.7693588676103247\n",
      "Epoch 8/350: Train loss 1.2515140771865845, Val loss 1.215101957321167, Train acc 0.7779764032892384 , Val acc 0.7768526228143214, F1-Score 0.7768526228143214\n",
      "Epoch 9/350: Train loss 1.2047778367996216, Val loss 1.1710762977600098, Train acc 0.7851269217018234 , Val acc 0.7893422148209825, F1-Score 0.7893422148209825\n",
      "Epoch 10/350: Train loss 1.1680060625076294, Val loss 1.1492432355880737, Train acc 0.791919914193779 , Val acc 0.7951706910907577, F1-Score 0.7951706910907577\n",
      "Epoch 11/350: Train loss 1.1485118865966797, Val loss 1.1349157094955444, Train acc 0.7965677511619592 , Val acc 0.794338051623647, F1-Score 0.794338051623647\n",
      "Epoch 12/350: Train loss 1.1284500360488892, Val loss 1.1283211708068848, Train acc 0.7979978548444763 , Val acc 0.79766860949209, F1-Score 0.7976686094920901\n",
      "Epoch 13/350: Train loss 1.1208693981170654, Val loss 1.1176506280899048, Train acc 0.8008580622095102 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 14/350: Train loss 1.1163427829742432, Val loss 1.116982340812683, Train acc 0.7994279585269932 , Val acc 0.79766860949209, F1-Score 0.7976686094920901\n",
      "Epoch 15/350: Train loss 1.1100894212722778, Val loss 1.113688349723816, Train acc 0.8008580622095102 , Val acc 0.79766860949209, F1-Score 0.7976686094920901\n",
      "Epoch 16/350: Train loss 1.109068751335144, Val loss 1.1101099252700806, Train acc 0.8015731140507687 , Val acc 0.7993338884263114, F1-Score 0.7993338884263114\n",
      "Epoch 17/350: Train loss 1.1078753471374512, Val loss 1.110093116760254, Train acc 0.8012155881301395 , Val acc 0.7993338884263114, F1-Score 0.7993338884263114\n",
      "Epoch 18/350: Train loss 1.1062010526657104, Val loss 1.1085528135299683, Train acc 0.8012155881301395 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 19/350: Train loss 1.1055313348770142, Val loss 1.1080152988433838, Train acc 0.8012155881301395 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 20/350: Train loss 1.1054363250732422, Val loss 1.1072884798049927, Train acc 0.8008580622095102 , Val acc 0.7993338884263114, F1-Score 0.7993338884263114\n",
      "Epoch 21/350: Train loss 1.1047983169555664, Val loss 1.1079270839691162, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 22/350: Train loss 1.104772925376892, Val loss 1.10769522190094, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 23/350: Train loss 1.104345679283142, Val loss 1.1065343618392944, Train acc 0.8012155881301395 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 24/350: Train loss 1.1046032905578613, Val loss 1.1066733598709106, Train acc 0.8012155881301395 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 25/350: Train loss 1.1039161682128906, Val loss 1.106386661529541, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 26/350: Train loss 1.1037745475769043, Val loss 1.1064856052398682, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 27/350: Train loss 1.1036707162857056, Val loss 1.1070386171340942, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 28/350: Train loss 1.1037116050720215, Val loss 1.1063706874847412, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 29/350: Train loss 1.1037179231643677, Val loss 1.105727195739746, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 30/350: Train loss 1.1035937070846558, Val loss 1.1059134006500244, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 31/350: Train loss 1.1035423278808594, Val loss 1.1060304641723633, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 32/350: Train loss 1.1034907102584839, Val loss 1.1061649322509766, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 33/350: Train loss 1.1034486293792725, Val loss 1.1054679155349731, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 34/350: Train loss 1.1033920049667358, Val loss 1.1061898469924927, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 35/350: Train loss 1.1038342714309692, Val loss 1.1054093837738037, Train acc 0.8012155881301395 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 36/350: Train loss 1.1036310195922852, Val loss 1.105446219444275, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 37/350: Train loss 1.1033512353897095, Val loss 1.1061725616455078, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 38/350: Train loss 1.1034427881240845, Val loss 1.105733036994934, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 39/350: Train loss 1.103333592414856, Val loss 1.1055521965026855, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 40/350: Train loss 1.1033576726913452, Val loss 1.1054476499557495, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 41/350: Train loss 1.1033837795257568, Val loss 1.10527765750885, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 42/350: Train loss 1.1035014390945435, Val loss 1.1054470539093018, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 43/350: Train loss 1.1033823490142822, Val loss 1.1054903268814087, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 44/350: Train loss 1.1034053564071655, Val loss 1.104996919631958, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 45/350: Train loss 1.1033424139022827, Val loss 1.1052823066711426, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 46/350: Train loss 1.1033542156219482, Val loss 1.105558156967163, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 47/350: Train loss 1.1035813093185425, Val loss 1.1053212881088257, Train acc 0.8012155881301395 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 48/350: Train loss 1.1033178567886353, Val loss 1.1051827669143677, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 49/350: Train loss 1.1033891439437866, Val loss 1.105642318725586, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 50/350: Train loss 1.10342276096344, Val loss 1.1051714420318604, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 51/350: Train loss 1.1034624576568604, Val loss 1.1049652099609375, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 52/350: Train loss 1.1033918857574463, Val loss 1.1050862073898315, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 53/350: Train loss 1.1033577919006348, Val loss 1.1054139137268066, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 54/350: Train loss 1.1035524606704712, Val loss 1.1050676107406616, Train acc 0.8012155881301395 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 55/350: Train loss 1.1033909320831299, Val loss 1.1049907207489014, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 56/350: Train loss 1.1033790111541748, Val loss 1.1054906845092773, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 57/350: Train loss 1.1032896041870117, Val loss 1.1053630113601685, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 58/350: Train loss 1.1033293008804321, Val loss 1.105298399925232, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 59/350: Train loss 1.1034860610961914, Val loss 1.1050457954406738, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 60/350: Train loss 1.1034127473831177, Val loss 1.1054049730300903, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 61/350: Train loss 1.1033459901809692, Val loss 1.1052461862564087, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 62/350: Train loss 1.1033393144607544, Val loss 1.105330467224121, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 63/350: Train loss 1.1033293008804321, Val loss 1.1051900386810303, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 64/350: Train loss 1.1033390760421753, Val loss 1.1050136089324951, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 65/350: Train loss 1.1033328771591187, Val loss 1.1051876544952393, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 66/350: Train loss 1.1033521890640259, Val loss 1.1050790548324585, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 67/350: Train loss 1.1033498048782349, Val loss 1.1048697233200073, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 68/350: Train loss 1.1034740209579468, Val loss 1.1050721406936646, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 69/350: Train loss 1.1033480167388916, Val loss 1.1053571701049805, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 70/350: Train loss 1.1032915115356445, Val loss 1.1052262783050537, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 71/350: Train loss 1.1033363342285156, Val loss 1.1053520441055298, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 72/350: Train loss 1.1033157110214233, Val loss 1.10537588596344, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 73/350: Train loss 1.1033096313476562, Val loss 1.1052262783050537, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 74/350: Train loss 1.1033263206481934, Val loss 1.105106234550476, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 75/350: Train loss 1.1032973527908325, Val loss 1.1052131652832031, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 76/350: Train loss 1.1032963991165161, Val loss 1.105296015739441, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 77/350: Train loss 1.10343337059021, Val loss 1.105378270149231, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 78/350: Train loss 1.103257417678833, Val loss 1.1051654815673828, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 79/350: Train loss 1.1032885313034058, Val loss 1.105100154876709, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 80/350: Train loss 1.1033087968826294, Val loss 1.1052219867706299, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 81/350: Train loss 1.1033296585083008, Val loss 1.1051675081253052, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 82/350: Train loss 1.1032896041870117, Val loss 1.1055169105529785, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 83/350: Train loss 1.1033354997634888, Val loss 1.1050490140914917, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 84/350: Train loss 1.1032934188842773, Val loss 1.1051660776138306, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 85/350: Train loss 1.1032911539077759, Val loss 1.1050537824630737, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 86/350: Train loss 1.1033159494400024, Val loss 1.1050487756729126, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 87/350: Train loss 1.1034013032913208, Val loss 1.1049085855484009, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 88/350: Train loss 1.1032778024673462, Val loss 1.1051616668701172, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 89/350: Train loss 1.1033140420913696, Val loss 1.1050918102264404, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 90/350: Train loss 1.1032708883285522, Val loss 1.1049549579620361, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 91/350: Train loss 1.1032739877700806, Val loss 1.105037808418274, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 92/350: Train loss 1.103316307067871, Val loss 1.1051394939422607, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 93/350: Train loss 1.1033467054367065, Val loss 1.1051394939422607, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 94/350: Train loss 1.1033272743225098, Val loss 1.1050570011138916, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 95/350: Train loss 1.1037192344665527, Val loss 1.104899287223816, Train acc 0.8012155881301395 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 96/350: Train loss 1.1033082008361816, Val loss 1.105183720588684, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 97/350: Train loss 1.1033202409744263, Val loss 1.1050583124160767, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 98/350: Train loss 1.1033987998962402, Val loss 1.1051955223083496, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 99/350: Train loss 1.1033002138137817, Val loss 1.105940818786621, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 100/350: Train loss 1.1034177541732788, Val loss 1.1049878597259521, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 101/350: Train loss 1.1033949851989746, Val loss 1.1051356792449951, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 102/350: Train loss 1.1032761335372925, Val loss 1.1053739786148071, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 103/350: Train loss 1.1032978296279907, Val loss 1.1052632331848145, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 104/350: Train loss 1.1033213138580322, Val loss 1.1054953336715698, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 105/350: Train loss 1.1033016443252563, Val loss 1.1048548221588135, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 106/350: Train loss 1.1032904386520386, Val loss 1.105094313621521, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 107/350: Train loss 1.103329062461853, Val loss 1.1050915718078613, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 108/350: Train loss 1.103377342224121, Val loss 1.105231523513794, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 109/350: Train loss 1.1033012866973877, Val loss 1.1050370931625366, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 110/350: Train loss 1.1033270359039307, Val loss 1.1050939559936523, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 111/350: Train loss 1.103568196296692, Val loss 1.105114459991455, Train acc 0.8012155881301395 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 112/350: Train loss 1.1032700538635254, Val loss 1.1049662828445435, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 113/350: Train loss 1.10328209400177, Val loss 1.105401635169983, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 114/350: Train loss 1.1033836603164673, Val loss 1.105149745941162, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 115/350: Train loss 1.1032652854919434, Val loss 1.1050617694854736, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 116/350: Train loss 1.1033059358596802, Val loss 1.1050825119018555, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 117/350: Train loss 1.1033333539962769, Val loss 1.1051145792007446, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 118/350: Train loss 1.1032694578170776, Val loss 1.1053259372711182, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 119/350: Train loss 1.1032671928405762, Val loss 1.1050152778625488, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 120/350: Train loss 1.1035497188568115, Val loss 1.105296015739441, Train acc 0.8012155881301395 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 121/350: Train loss 1.1032856702804565, Val loss 1.1049987077713013, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 122/350: Train loss 1.1033058166503906, Val loss 1.1051607131958008, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 123/350: Train loss 1.1032634973526, Val loss 1.1052305698394775, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 124/350: Train loss 1.1033520698547363, Val loss 1.1050429344177246, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 125/350: Train loss 1.1032779216766357, Val loss 1.1048705577850342, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 126/350: Train loss 1.1032735109329224, Val loss 1.105129599571228, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 127/350: Train loss 1.103268027305603, Val loss 1.1050655841827393, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 128/350: Train loss 1.1032991409301758, Val loss 1.1049089431762695, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 129/350: Train loss 1.1032743453979492, Val loss 1.1049416065216064, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 130/350: Train loss 1.1032973527908325, Val loss 1.104993462562561, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 131/350: Train loss 1.1033060550689697, Val loss 1.1048591136932373, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 132/350: Train loss 1.1032757759094238, Val loss 1.1048554182052612, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 133/350: Train loss 1.1032623052597046, Val loss 1.1051063537597656, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 134/350: Train loss 1.1032764911651611, Val loss 1.104910135269165, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 135/350: Train loss 1.1033594608306885, Val loss 1.1050262451171875, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 136/350: Train loss 1.1032979488372803, Val loss 1.1050496101379395, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 137/350: Train loss 1.1032702922821045, Val loss 1.1049333810806274, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 138/350: Train loss 1.1032716035842896, Val loss 1.1049963235855103, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 139/350: Train loss 1.1032776832580566, Val loss 1.1049418449401855, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 140/350: Train loss 1.1032729148864746, Val loss 1.1048966646194458, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 141/350: Train loss 1.103281021118164, Val loss 1.1049044132232666, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 142/350: Train loss 1.103281021118164, Val loss 1.1052030324935913, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 143/350: Train loss 1.1032822132110596, Val loss 1.1049541234970093, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 144/350: Train loss 1.1032754182815552, Val loss 1.1049891710281372, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 145/350: Train loss 1.103460669517517, Val loss 1.105118751525879, Train acc 0.8012155881301395 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 146/350: Train loss 1.103268027305603, Val loss 1.1058597564697266, Train acc 0.8015731140507687 , Val acc 0.7993338884263114, F1-Score 0.7993338884263114\n",
      "Epoch 147/350: Train loss 1.103276252746582, Val loss 1.1048974990844727, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 148/350: Train loss 1.1034713983535767, Val loss 1.1051154136657715, Train acc 0.8012155881301395 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 149/350: Train loss 1.1032670736312866, Val loss 1.1050810813903809, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 150/350: Train loss 1.1032739877700806, Val loss 1.1053587198257446, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 151/350: Train loss 1.103267788887024, Val loss 1.1047922372817993, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 152/350: Train loss 1.1032629013061523, Val loss 1.105010747909546, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 153/350: Train loss 1.1032774448394775, Val loss 1.1048691272735596, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 154/350: Train loss 1.103271245956421, Val loss 1.1049917936325073, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 155/350: Train loss 1.103267788887024, Val loss 1.1048129796981812, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 156/350: Train loss 1.1032689809799194, Val loss 1.1048758029937744, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 157/350: Train loss 1.1032896041870117, Val loss 1.1047574281692505, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 158/350: Train loss 1.1033445596694946, Val loss 1.104997992515564, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 159/350: Train loss 1.1032683849334717, Val loss 1.105076551437378, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 160/350: Train loss 1.1033482551574707, Val loss 1.1049975156784058, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 161/350: Train loss 1.1032681465148926, Val loss 1.1051149368286133, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 162/350: Train loss 1.1032700538635254, Val loss 1.1049306392669678, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 163/350: Train loss 1.103265404701233, Val loss 1.1047375202178955, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 164/350: Train loss 1.1032735109329224, Val loss 1.1048356294631958, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 165/350: Train loss 1.1032758951187134, Val loss 1.1050599813461304, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 166/350: Train loss 1.1032735109329224, Val loss 1.1050282716751099, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 167/350: Train loss 1.103288173675537, Val loss 1.105010986328125, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 168/350: Train loss 1.103865623474121, Val loss 1.1048228740692139, Train acc 0.8008580622095102 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 169/350: Train loss 1.1034266948699951, Val loss 1.1050068140029907, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 170/350: Train loss 1.1032606363296509, Val loss 1.1051019430160522, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 171/350: Train loss 1.1032623052597046, Val loss 1.105278730392456, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 172/350: Train loss 1.1032729148864746, Val loss 1.1049094200134277, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 173/350: Train loss 1.1032731533050537, Val loss 1.1049915552139282, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 174/350: Train loss 1.1032663583755493, Val loss 1.104668378829956, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 175/350: Train loss 1.103261113166809, Val loss 1.105147123336792, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 176/350: Train loss 1.103268027305603, Val loss 1.104973316192627, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 177/350: Train loss 1.103277564048767, Val loss 1.104807734489441, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 178/350: Train loss 1.1036072969436646, Val loss 1.1048808097839355, Train acc 0.8012155881301395 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 179/350: Train loss 1.1033155918121338, Val loss 1.1049379110336304, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 180/350: Train loss 1.1032625436782837, Val loss 1.1048237085342407, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 181/350: Train loss 1.103317141532898, Val loss 1.1049631834030151, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 182/350: Train loss 1.103264570236206, Val loss 1.1048345565795898, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 183/350: Train loss 1.1032979488372803, Val loss 1.1049270629882812, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 184/350: Train loss 1.1032612323760986, Val loss 1.104980707168579, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 185/350: Train loss 1.1032633781433105, Val loss 1.104764461517334, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 186/350: Train loss 1.1032599210739136, Val loss 1.1050995588302612, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 187/350: Train loss 1.1032847166061401, Val loss 1.104804277420044, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 188/350: Train loss 1.1032614707946777, Val loss 1.1049202680587769, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 189/350: Train loss 1.1032674312591553, Val loss 1.1049227714538574, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 190/350: Train loss 1.1032596826553345, Val loss 1.1048550605773926, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 191/350: Train loss 1.1032661199569702, Val loss 1.1048531532287598, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 192/350: Train loss 1.1032625436782837, Val loss 1.1050456762313843, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 193/350: Train loss 1.1032686233520508, Val loss 1.1048132181167603, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 194/350: Train loss 1.1032612323760986, Val loss 1.105001449584961, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 195/350: Train loss 1.103387475013733, Val loss 1.1049168109893799, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 196/350: Train loss 1.1032713651657104, Val loss 1.1048885583877563, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 197/350: Train loss 1.1032617092132568, Val loss 1.104933261871338, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 198/350: Train loss 1.103266716003418, Val loss 1.1049152612686157, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 199/350: Train loss 1.1032633781433105, Val loss 1.1049336194992065, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 200/350: Train loss 1.1032965183258057, Val loss 1.1048152446746826, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 201/350: Train loss 1.1032706499099731, Val loss 1.1049329042434692, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 202/350: Train loss 1.1032928228378296, Val loss 1.1048450469970703, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 203/350: Train loss 1.103278398513794, Val loss 1.104856014251709, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 204/350: Train loss 1.1032694578170776, Val loss 1.104891061782837, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 205/350: Train loss 1.1033042669296265, Val loss 1.104934573173523, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 206/350: Train loss 1.1032764911651611, Val loss 1.1049538850784302, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 207/350: Train loss 1.1032781600952148, Val loss 1.1048798561096191, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 208/350: Train loss 1.1032640933990479, Val loss 1.104849100112915, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 209/350: Train loss 1.1032731533050537, Val loss 1.1050654649734497, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 210/350: Train loss 1.1032614707946777, Val loss 1.1050026416778564, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 211/350: Train loss 1.1032612323760986, Val loss 1.104899525642395, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 212/350: Train loss 1.1032629013061523, Val loss 1.1049686670303345, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 213/350: Train loss 1.1032624244689941, Val loss 1.1047674417495728, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 214/350: Train loss 1.1032779216766357, Val loss 1.104750156402588, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 215/350: Train loss 1.1035223007202148, Val loss 1.1047751903533936, Train acc 0.8012155881301395 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 216/350: Train loss 1.1032729148864746, Val loss 1.104866623878479, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 217/350: Train loss 1.1032602787017822, Val loss 1.1051822900772095, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 218/350: Train loss 1.103270173072815, Val loss 1.1048845052719116, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 219/350: Train loss 1.1032631397247314, Val loss 1.1049014329910278, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 220/350: Train loss 1.1033058166503906, Val loss 1.104933261871338, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 221/350: Train loss 1.103273868560791, Val loss 1.1049524545669556, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 222/350: Train loss 1.1032700538635254, Val loss 1.1047815084457397, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 223/350: Train loss 1.10326087474823, Val loss 1.1047723293304443, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 224/350: Train loss 1.1032670736312866, Val loss 1.104790210723877, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 225/350: Train loss 1.1034910678863525, Val loss 1.1048890352249146, Train acc 0.8012155881301395 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 226/350: Train loss 1.1032682657241821, Val loss 1.1048383712768555, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 227/350: Train loss 1.1032798290252686, Val loss 1.104980707168579, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 228/350: Train loss 1.1032625436782837, Val loss 1.1049028635025024, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 229/350: Train loss 1.1032629013061523, Val loss 1.1049604415893555, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 230/350: Train loss 1.1032862663269043, Val loss 1.1047546863555908, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 231/350: Train loss 1.1032615900039673, Val loss 1.1048717498779297, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 232/350: Train loss 1.1032699346542358, Val loss 1.105038046836853, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 233/350: Train loss 1.1032778024673462, Val loss 1.1047509908676147, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 234/350: Train loss 1.103312373161316, Val loss 1.1049798727035522, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 235/350: Train loss 1.1032637357711792, Val loss 1.1050242185592651, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 236/350: Train loss 1.103295087814331, Val loss 1.104851245880127, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 237/350: Train loss 1.103261113166809, Val loss 1.1049407720565796, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 238/350: Train loss 1.1032642126083374, Val loss 1.104804515838623, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 239/350: Train loss 1.1032804250717163, Val loss 1.1047190427780151, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 240/350: Train loss 1.1032696962356567, Val loss 1.1049634218215942, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 241/350: Train loss 1.1032599210739136, Val loss 1.104874610900879, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 242/350: Train loss 1.1032596826553345, Val loss 1.104705810546875, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 243/350: Train loss 1.1032661199569702, Val loss 1.1049118041992188, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 244/350: Train loss 1.1032885313034058, Val loss 1.1047312021255493, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 245/350: Train loss 1.103380799293518, Val loss 1.1049922704696655, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 246/350: Train loss 1.1032551527023315, Val loss 1.1047495603561401, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 247/350: Train loss 1.103266716003418, Val loss 1.1048463582992554, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 248/350: Train loss 1.1032600402832031, Val loss 1.1050565242767334, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 249/350: Train loss 1.1032633781433105, Val loss 1.1046949625015259, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 250/350: Train loss 1.1032607555389404, Val loss 1.1057180166244507, Train acc 0.8015731140507687 , Val acc 0.7993338884263114, F1-Score 0.7993338884263114\n",
      "Epoch 251/350: Train loss 1.103284478187561, Val loss 1.1047438383102417, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 252/350: Train loss 1.1032735109329224, Val loss 1.1050963401794434, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 253/350: Train loss 1.1032794713974, Val loss 1.1049548387527466, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 254/350: Train loss 1.1032588481903076, Val loss 1.1048182249069214, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 255/350: Train loss 1.1032629013061523, Val loss 1.104813575744629, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 256/350: Train loss 1.1032600402832031, Val loss 1.1048251390457153, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 257/350: Train loss 1.1032739877700806, Val loss 1.104945421218872, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 258/350: Train loss 1.1032633781433105, Val loss 1.10469651222229, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 259/350: Train loss 1.1032696962356567, Val loss 1.1048946380615234, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 260/350: Train loss 1.1032664775848389, Val loss 1.1047996282577515, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 261/350: Train loss 1.1032638549804688, Val loss 1.1048758029937744, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 262/350: Train loss 1.1032800674438477, Val loss 1.1049394607543945, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 263/350: Train loss 1.1032617092132568, Val loss 1.1049026250839233, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 264/350: Train loss 1.1032626628875732, Val loss 1.1047799587249756, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 265/350: Train loss 1.1032634973526, Val loss 1.105095624923706, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 266/350: Train loss 1.1032634973526, Val loss 1.1047619581222534, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 267/350: Train loss 1.103265643119812, Val loss 1.104799509048462, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 268/350: Train loss 1.1032649278640747, Val loss 1.1048038005828857, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 269/350: Train loss 1.103330373764038, Val loss 1.105058193206787, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 270/350: Train loss 1.1032626628875732, Val loss 1.1049939393997192, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 271/350: Train loss 1.1032648086547852, Val loss 1.1047585010528564, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 272/350: Train loss 1.103286623954773, Val loss 1.1049127578735352, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 273/350: Train loss 1.1032623052597046, Val loss 1.1047435998916626, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 274/350: Train loss 1.1032993793487549, Val loss 1.1048030853271484, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 275/350: Train loss 1.1032633781433105, Val loss 1.1051465272903442, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 276/350: Train loss 1.1032660007476807, Val loss 1.1049792766571045, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 277/350: Train loss 1.1034654378890991, Val loss 1.1048251390457153, Train acc 0.8012155881301395 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 278/350: Train loss 1.1032716035842896, Val loss 1.1047769784927368, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 279/350: Train loss 1.10326087474823, Val loss 1.1048674583435059, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 280/350: Train loss 1.1032651662826538, Val loss 1.104711890220642, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 281/350: Train loss 1.1032657623291016, Val loss 1.104852557182312, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 282/350: Train loss 1.1032602787017822, Val loss 1.1050357818603516, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 283/350: Train loss 1.1032634973526, Val loss 1.1048550605773926, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 284/350: Train loss 1.1032640933990479, Val loss 1.1047476530075073, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 285/350: Train loss 1.103262186050415, Val loss 1.1049833297729492, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 286/350: Train loss 1.1032664775848389, Val loss 1.1047247648239136, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 287/350: Train loss 1.1032624244689941, Val loss 1.1047090291976929, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 288/350: Train loss 1.1033028364181519, Val loss 1.104844093322754, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 289/350: Train loss 1.1032600402832031, Val loss 1.1048352718353271, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 290/350: Train loss 1.103257179260254, Val loss 1.1048340797424316, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 291/350: Train loss 1.103270173072815, Val loss 1.104836344718933, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 292/350: Train loss 1.1032605171203613, Val loss 1.104756474494934, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 293/350: Train loss 1.1032651662826538, Val loss 1.104882836341858, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 294/350: Train loss 1.103261113166809, Val loss 1.104853868484497, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 295/350: Train loss 1.1032602787017822, Val loss 1.10518217086792, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 296/350: Train loss 1.103261113166809, Val loss 1.1047800779342651, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 297/350: Train loss 1.103269338607788, Val loss 1.1050034761428833, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 298/350: Train loss 1.1034187078475952, Val loss 1.104996919631958, Train acc 0.8012155881301395 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 299/350: Train loss 1.1032600402832031, Val loss 1.104733943939209, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 300/350: Train loss 1.1032603979110718, Val loss 1.1048318147659302, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 301/350: Train loss 1.1032603979110718, Val loss 1.104932427406311, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 302/350: Train loss 1.1032624244689941, Val loss 1.104891061782837, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 303/350: Train loss 1.1032605171203613, Val loss 1.1049166917800903, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 304/350: Train loss 1.1032599210739136, Val loss 1.1049658060073853, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 305/350: Train loss 1.1032559871673584, Val loss 1.1047344207763672, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 306/350: Train loss 1.1032602787017822, Val loss 1.104776382446289, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 307/350: Train loss 1.1032791137695312, Val loss 1.1049107313156128, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 308/350: Train loss 1.1032618284225464, Val loss 1.1049389839172363, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 309/350: Train loss 1.1032646894454956, Val loss 1.104763150215149, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 310/350: Train loss 1.1032806634902954, Val loss 1.1048954725265503, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 311/350: Train loss 1.1032599210739136, Val loss 1.104766607284546, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 312/350: Train loss 1.1032642126083374, Val loss 1.1047827005386353, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 313/350: Train loss 1.1032612323760986, Val loss 1.104823112487793, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 314/350: Train loss 1.103259563446045, Val loss 1.1048511266708374, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 315/350: Train loss 1.1032600402832031, Val loss 1.104766845703125, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 316/350: Train loss 1.1033014059066772, Val loss 1.1048201322555542, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 317/350: Train loss 1.1032644510269165, Val loss 1.1047792434692383, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 318/350: Train loss 1.1032617092132568, Val loss 1.104978084564209, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 319/350: Train loss 1.1032614707946777, Val loss 1.1047648191452026, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 320/350: Train loss 1.1033176183700562, Val loss 1.1049193143844604, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 321/350: Train loss 1.1032605171203613, Val loss 1.1049891710281372, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 322/350: Train loss 1.1032596826553345, Val loss 1.1048147678375244, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 323/350: Train loss 1.1032603979110718, Val loss 1.1047648191452026, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 324/350: Train loss 1.1032640933990479, Val loss 1.1048816442489624, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 325/350: Train loss 1.1032664775848389, Val loss 1.1048462390899658, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 326/350: Train loss 1.1032967567443848, Val loss 1.1049495935440063, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 327/350: Train loss 1.1032791137695312, Val loss 1.1048188209533691, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 328/350: Train loss 1.1032649278640747, Val loss 1.1048829555511475, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 329/350: Train loss 1.10326087474823, Val loss 1.10482919216156, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 330/350: Train loss 1.1032637357711792, Val loss 1.1049277782440186, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 331/350: Train loss 1.1032652854919434, Val loss 1.104982852935791, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 332/350: Train loss 1.10326087474823, Val loss 1.1047884225845337, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 333/350: Train loss 1.1032596826553345, Val loss 1.1048029661178589, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 334/350: Train loss 1.103280782699585, Val loss 1.1047568321228027, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 335/350: Train loss 1.1033796072006226, Val loss 1.1047594547271729, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 336/350: Train loss 1.103265404701233, Val loss 1.1047974824905396, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 337/350: Train loss 1.1032631397247314, Val loss 1.1049070358276367, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 338/350: Train loss 1.1032605171203613, Val loss 1.1047579050064087, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 339/350: Train loss 1.1032618284225464, Val loss 1.1048054695129395, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 340/350: Train loss 1.1032618284225464, Val loss 1.104967713356018, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 341/350: Train loss 1.1032694578170776, Val loss 1.1047966480255127, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 342/350: Train loss 1.1032642126083374, Val loss 1.1049551963806152, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 343/350: Train loss 1.1032596826553345, Val loss 1.1047025918960571, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 344/350: Train loss 1.1032663583755493, Val loss 1.1049329042434692, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 345/350: Train loss 1.103265643119812, Val loss 1.1047613620758057, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 346/350: Train loss 1.1032634973526, Val loss 1.104796051979065, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 347/350: Train loss 1.10326087474823, Val loss 1.1050853729248047, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 348/350: Train loss 1.1032626628875732, Val loss 1.1047757863998413, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 349/350: Train loss 1.1032651662826538, Val loss 1.1048296689987183, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Epoch 350/350: Train loss 1.1032629013061523, Val loss 1.1049305200576782, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n",
      "Confusion Matrix :\n",
      "[[  0   0   0   0  59]\n",
      " [  0   0   0   0  19]\n",
      " [  0   0   0   0  32]\n",
      " [  0   0   0   0 130]\n",
      " [  0   0   0   0 961]]\n",
      "\n",
      "Epoch 350/350: Train loss 1.1032629013061523, Val loss 1.1049305200576782, Train acc 0.8015731140507687 , Val acc 0.8001665278934221, F1-Score 0.8001665278934222\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = 3\n",
    "X_train, X_val, y_train, y_val = getDataset(dataset)\n",
    "\n",
    "train_data = (X_train,y_train)\n",
    "val_data = (X_val,y_val)\n",
    "\n",
    "\n",
    "input_dim = glove_embeddings.d_emb\n",
    "hidden1_dim = 200\n",
    "hidden2_dim = 100\n",
    "hidden3_dim = 50 \n",
    "drop_out1 = 0.5\n",
    "drop_out2 = 0.5\n",
    "drop_out3 = 0.5\n",
    "num_calsses = 3 if dataset==1 else 2 if dataset==2 else 5\n",
    "\n",
    "\n",
    "epochs = 350\n",
    "lr = 0.001\n",
    "\n",
    "\n",
    "\n",
    "model = DANSentimentClassifier(input_dim,hidden1_dim,hidden2_dim,hidden3_dim,drop_out1,drop_out2,drop_out3,num_calsses)\n",
    "\n",
    "\n",
    "train(model,train_data,val_data,epochs,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
